@article{carlsmith2022power,
  title={Is Power-Seeking AI an Existential Risk?},
  author={Carlsmith, Joseph},
  journal={arXiv preprint arXiv:2206.13353},
  year={2022}
}

@article{hendrycks2021aligning,
  title={Aligning AI With Shared Human Values},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew and Li, Jerry and Song, Dawn and Steinhardt, Jacob},
  journal={Proceedings of the International Conference on Learning Representations},
  year={2021}
}

@article{perez2022discovering,
  title={Discovering Language Model Behaviors with Model-Written Evaluations},
  author={Perez, Ethan and Ringer, Sam and Lukošiūtė, Kamile and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and Jones, Andy and Chen, Anna and Mann, Ben and Ask, Brianna and Yeh, Amanda and Das, Sheer and Dusenberry, Micheal and Brockman, Amanda and Sutskever, Ilya and Amodei, Dario},
  journal={arXiv preprint arXiv:2212.09251},
  year={2022}
}

@article{anthropic2025agentic,
  title={Agentic Misalignment: How LLMs could be insider threats},
  author={Anthropic},
  journal={arXiv preprint arXiv:2406.20053},
  year={2025}
}

@article{chiu2025will,
  title={Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas},
  author={Chiu, Yu Ying and Wang, Zhilin and Maiya, Sharan and Choi, Yejin and Fish, Kyle and Levine, Sydney and Hubinger, Evan},
  journal={arXiv preprint arXiv:2505.14633},
  year={2025}
}

@article{aldahoul2025large,
  title={Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts},
  author={Aldahoul, Nouar and Ibrahim, Hazem and Varvello, Matteo and Kaufman, Aaron and Rahwan, Talal and Zaki, Yasir},
  journal={arXiv preprint arXiv:2505.04171},
  year={2025}
}

@article{zhou2025ai,
  title={AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents},
  author={Zhou, Xuhui and Sap, Maarten},
  journal={Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year={2025}
}

@article{chakraborty2025structured,
  title={Structured Moral Reasoning in Language Models: A Value-Grounded Evaluation Framework},
  author={Chakraborty, Mohna and Wang, Lu and Jurgens, David},
  journal={arXiv preprint arXiv:2506.14948},
  year={2025}
}

@article{liu2025truth,
  title={TRUTH DECAY: Quantifying Multi-Turn Sycophancy in Language Models},
  author={Liu, Joshua and Jain, Aarav and Takuri, Soham and Vege, Srihan and Akalin, Aslihan and Zhu, Kevin and O'Brien, Sean and Sharma, Vasu},
  journal={arXiv preprint arXiv:2503.11656},
  year={2025}
}

@article{saju2025facts,
  title={Facts are Harder Than Opinions -- A Multilingual, Comparative Analysis of LLM-Based Fact-Checking Reliability},
  author={Saju, Lorraine and Bleier, Arnim and Lasser, Jana and Wagner, Claudia},
  journal={arXiv preprint arXiv:2506.03655},
  year={2025}
}

@article{proma2025empirical,
  title={An Empirical Analysis of LLMs for Countering Misinformation},
  author={Proma, Adiba Mahbub and Pate, Neeley and Druckman, James and Ghoshal, Gourab and He, Hangfeng and Hoque, Ehsan},
  journal={arXiv preprint arXiv:2503.01902},
  year={2025}
}

@article{curtis2025veracity,
  title={Veracity: An Open-Source AI Fact-Checking System},
  author={Curtis, Taylor Lynn and Touzel, Maximilian Puelma and Garneau, William and Gruaz, Manon and Pinder, Mike and Wang, Li Wei and Krishna, Sukanya and Cohen, Luda and Godbout, Jean-François and Rabbany, Reihaneh and Pelrine, Kellin},
  journal={arXiv preprint arXiv:2506.15794},
  year={2025}
}

@article{shukla2025recon,
  title={Recon, Answer, Verify: Agents in Search of Truth},
  author={Shukla, Satyam and Dutta, Himanshu and Bhattacharyya, Pushpak},
  journal={arXiv preprint arXiv:2507.03671},
  year={2025}
}

% Placeholder entries for papers that need to be found - these should be replaced with actual citations
@article{mazeika2025ethics,
  title={Ethics in AI: Evaluating Model Values in Decision-Making},
  author={Mazeika, Mantas and others},
  journal={[To be replaced with actual citation]},
  year={2025}
}

@article{betley2025situational,
  title={Situational Awareness and Deception in AI Systems},
  author={Betley, Sam and others},
  journal={[To be replaced with actual citation]},
  year={2025}
}

@article{claude4systemcard2025,
  title={Claude 4 System Card},
  author={Anthropic},
  journal={[To be replaced with actual citation]},
  year={2025}
}

@article{jiao2025ethics,
  title={Ethics in LLM Evaluation},
  author={Jiao, Wenlong and others},
  journal={[To be replaced with actual citation]},
  year={2025}
}

@article{salecha2025political,
  title={Political Bias in Language Models},
  author={Salecha, Arjun and others},
  journal={[To be replaced with actual citation]},
  year={2025}
}

@article{hubinger2025sleeper,
  title={Sleeper Agents in AI Systems},
  author={Hubinger, Evan and others},
  journal={[To be replaced with actual citation]},
  year={2025}
}

@article{mazeika2025utility,
  title={Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs},
  author={Mazeika, Mantas and Yin, Xuwang and Tamirisa, Rishub and Lim, Jaehyuk and Lee, Bruce W and Ren, Richard and Phan, Long and Mu, Norman and Khoja, Adam and Zhang, Oliver and others},
  journal={arXiv preprint arXiv:2502.08640},
  year={2025}
} 